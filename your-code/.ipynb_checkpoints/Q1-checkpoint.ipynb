{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create a Python function that wraps your previous solution for the Bag of Words lab.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. Your function should accept the following parameters:\n",
    "    * `docs` [REQUIRED] - array of document paths.\n",
    "    * `stop_words` [OPTIONAL] - array of stop words. The default value is an empty array.\n",
    "\n",
    "1. Your function should return a Python object that contains the following:\n",
    "    * `bag_of_words` - array of strings of normalized unique words in the corpus.\n",
    "    * `term_freq` - array of the term-frequency vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/greg/Desktop/Data lab Ironhack/data-labs/module-1/lab-string-operations/your-code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd5110563e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfile_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectorty_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/greg/Desktop/Data lab Ironhack/data-labs/module-1/lab-string-operations/your-code'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "L.S. I cannot acces the code. Can you change the directory path so that the list comprehension can be used? Please\n",
    "make sure that we can also access the files. \n",
    "'''\n",
    "\n",
    "\n",
    "# directorty_path = os.getcwd() # the generic function \n",
    "directorty_path = '/Users/greg/Desktop/Data lab Ironhack/data-labs/module-1/lab-string-operations/your-code'\n",
    "\n",
    "\n",
    "file_list = [file for file in os.listdir(directorty_path) if file.endswith('.txt')] \n",
    "\n",
    "print (file_list)\n",
    "\n",
    "file_dir_list = []\n",
    "\n",
    "for el in file_list: \n",
    "    full_name = os.path.join(directorty_path, el) #the path to join, the file name itself \n",
    "    file_dir_list.append(full_name)\n",
    "    \n",
    "print (file_dir_list)\n",
    "\n",
    "\n",
    "# Define function\n",
    "def get_bow_from_docs(docs, stop_words=[]):\n",
    "    \n",
    "    # In the function, first define the variables you will use such as `corpus`, `bag_of_words`, and `term_freq`.\n",
    "    \n",
    "    \"\"\"\n",
    "    Loop `docs` and read the content of each doc into a string in `corpus`.\n",
    "    Remember to convert the doc content to lowercases and remove punctuation.\n",
    "    \"\"\"\n",
    "    corpus =[]\n",
    "\n",
    "    for i in docs:\n",
    "        # prepare reading \n",
    "        file = open(i, \"r\")\n",
    "        # reading it and adding it to corpus list \n",
    "        corpus.append(file.read())\n",
    "    \n",
    "    import re\n",
    "\n",
    "    # list of list of docs : [[ list doc1],[ list doc2],[ list doc2]] \n",
    "    corpus1 =[]\n",
    "\n",
    "    for i in corpus:\n",
    "        x = (i.lower())\n",
    "        y =(re.findall('[\\w]+', x))\n",
    "        corpus1.append(y)\n",
    "        \n",
    "    corpus2 =[]\n",
    "\n",
    "    for i in corpus:\n",
    "        x = (i.lower())\n",
    "        y =(re.findall('[\\w]+', x))\n",
    "        corpus2 = corpus2 + y\n",
    "\n",
    "    # list of unique bag or words : [uniqueword1, uniqueword2, uniqueword3, ... ] \n",
    "    bag_of_words=[]\n",
    "\n",
    "    for i in corpus2:\n",
    "        if i not in bag_of_words:\n",
    "            bag_of_words.append(i)\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Loop `corpus`. Append the terms in each doc into the `bag_of_words` array. The terms in `bag_of_words` \n",
    "    should be unique which means before adding each term you need to check if it's already added to the array.\n",
    "    In addition, check if each term is in the `stop_words` array. Only append the term to `bag_of_words`\n",
    "    if it is not a stop word.\n",
    "    \"\"\"\n",
    "\n",
    "    # still to write....\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Loop `corpus` again. For each doc string, count the number of occurrences of each term in `bag_of_words`. \n",
    "    Create an array for each doc's term frequency and append it to `term_freq`.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    doc1 = []\n",
    "    doc2 = []\n",
    "    doc3 = []\n",
    "\n",
    "    term_freq = [doc1, doc2, doc3]\n",
    "\n",
    "    count = 0 \n",
    "\n",
    "    for word in bag_of_words:\n",
    "        for corp in corpus1:\n",
    "        \n",
    "            if count == 0:\n",
    "                count += 1\n",
    "                if word in corp:\n",
    "                    doc1.append(1)           \n",
    "                else:\n",
    "                    doc1.append(0)\n",
    "                    \n",
    "                \n",
    "            elif count == 1: \n",
    "                count += 1\n",
    "                if word in corp:\n",
    "                    doc2.append(1) \n",
    "                else:\n",
    "                    doc2.append(0)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                count = 0 \n",
    "                if word in corp:\n",
    "                    doc3.append(1)\n",
    "                else:\n",
    "                    doc3.append(0)\n",
    "    \n",
    "    # Now return your output as an object\n",
    "    return {\n",
    "        \"bag_of_words\": bag_of_words,\n",
    "        \"term_freq\": term_freq\n",
    "    }\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function without stop words. You should see the output like below:\n",
    "\n",
    "```{'bag_of_words': ['ironhack', 'is', 'cool', 'i', 'love', 'am', 'a', 'student', 'at'], 'term_freq': [[1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 0, 0, 1, 1, 0, 0, 0, 0], [1, 0, 0, 1, 0, 1, 1, 1, 1]]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_dir_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-63d4fdc1d59d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define doc paths array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_dir_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Obtain BoW from your function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bow_from_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_dir_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Define doc paths array\n",
    "docs = file_dir_list\n",
    "\n",
    "# Obtain BoW from your function\n",
    "bow = get_bow_from_docs(docs)\n",
    "\n",
    "# Print BoW\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your attempt above is successful, nice work done!\n",
    "\n",
    "Now test your function again with the stop words. In the previous lab we defined the stop words in a large array. In this lab, we'll import the stop words from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'beyond', 'interest', 'among', 'afterwards', 'call', 'beforehand', 'couldnt', 'either', 'due', 'everyone', 'another', 'he', 'herself', 'him', 'sometime', 'get', 'therein', 'these', 'wherein', 'my', 'inc', 'one', 'through', 'found', 'de', 'whose', 'for', 'six', 'why', 'eight', 'whenever', 'besides', 'yours', 'namely', 'she', 'below', 'etc', 'two', 'thus', 'yourself', 'con', 'there', 'perhaps', 'hence', 'those', 'from', 'down', 'cant', 'already', 'must', 'against', 'latterly', 'else', 'your', 'behind', 'made', 'might', 'nine', 'is', 'between', 'mill', 'four', 'go', 'has', 'eg', 'co', 'bill', 'off', 'myself', 'other', 'same', 'both', 'whatever', 'anything', 'can', 'only', 'have', 'own', 'becomes', 'please', 'amongst', 'formerly', 'onto', 'last', 'forty', 'former', 'describe', 'into', 'now', 'find', 'serious', 'thick', 'give', 'done', 'an', 'seeming', 'whoever', 'anyhow', 'them', 'three', 'before', 'they', 'becoming', 'us', 'along', 'upon', 'was', 'our', 'thin', 'twenty', 'hereby', 'anyway', 'around', 'everywhere', 'fifty', 'himself', 'sixty', 'up', 'side', 'we', 'nothing', 'always', 'whom', 'otherwise', 'than', 'sincere', 'keep', 'therefore', 'whereby', 'once', 'towards', 'whereupon', 'thence', 'front', 'bottom', 'any', 'during', 'via', 'until', 'anyone', 'empty', 'a', 'ever', 'less', 'who', 'nor', 'top', 'which', 'and', 'none', 'noone', 'in', 'become', 'most', 'about', 'move', 'on', 'so', 'after', 'show', 'still', 'first', 'five', 'fifteen', 'hasnt', 'whereas', 'much', 'well', 'yet', 'hers', 'amoungst', 'it', 'few', 'such', 'thereby', 'some', 'be', 'beside', 'take', 'alone', 'cry', 'anywhere', 'the', 'out', 'do', 'being', 'seemed', 'themselves', 'part', 'name', 'thereafter', 'whereafter', 'will', 'wherever', 'mine', 'several', 'system', 'thru', 'elsewhere', 'under', 'all', 'sometimes', 'fill', 'because', 'that', 'their', 'were', 'over', 'cannot', 'seem', 'itself', 'fire', 'detail', 'more', 'put', 'where', 'eleven', 'many', 'you', 'hereupon', 'above', 'un', 'everything', 'except', 'to', 'seems', 'may', 'her', 'since', 'very', 'twelve', 'when', 'although', 'me', 're', 'something', 'hundred', 'ten', 'third', 'yourselves', 'would', 'again', 'or', 'within', 'i', 'without', 'back', 'this', 'each', 'others', 'throughout', 'never', 'further', 'could', 'full', 'herein', 'though', 'whether', 'its', 'nevertheless', 'ie', 'no', 'hereafter', 'see', 'amount', 'been', 'not', 'per', 'what', 'am', 'by', 'whither', 'with', 'how', 'somewhere', 'enough', 'ourselves', 'least', 'ltd', 'ours', 'mostly', 'however', 'while', 'every', 'toward', 'somehow', 'but', 'had', 'here', 'of', 'should', 'someone', 'too', 'whence', 'next', 'as', 'also', 'meanwhile', 'then', 'at', 'indeed', 'together', 'almost', 'are', 'moreover', 'became', 'nowhere', 'nobody', 'across', 'neither', 'if', 'thereupon', 'even', 'often', 'whole', 'his', 'latter', 'rather'})\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "L.S. Yes, this works. \n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "print(stop_words.ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have seen a large list of words that looks like:\n",
    "\n",
    "```frozenset({'across', 'mine', 'cannot', ...})```\n",
    "\n",
    "`frozenset` is a type of Python object that is immutable. In this lab you can use it just like an array without conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, test your function with supplying `stop_words.ENGLISH_STOP_WORDS` as the second parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_bow_from_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-158d5d02aaa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bow_from_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENGLISH_STOP_WORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_bow_from_docs' is not defined"
     ]
    }
   ],
   "source": [
    "bow = get_bow_from_docs(bow, stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have seen:\n",
    "\n",
    "```{'bag_of_words': ['ironhack', 'cool', 'love', 'student'], 'term_freq': [[1, 1, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1]]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
